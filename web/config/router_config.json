{
  "max_model_size": "8GB",
  "preferred_quantization": "Q4_K_M",
  "specialization_weights": {
    "coding": 1.5,
    "programming": 1.5,
    "debugging": 1.4,
    "creative": 1.6,
    "general": 1.0,
    "math": 1.3,
    "reasoning": 1.4,
    "conversation": 1.1,
    "chat": 1.1,
    "multilingual": 1.2,
    "vision": 1.3,
    "multimodal": 1.3
  },
  "openai_meta_routing": {
    "enabled": true,
    "model": "gpt-4.1-mini",
    "api_key": null,
    "cache_decisions": true,
    "cache_duration_hours": 2,
    "timeout_seconds": 10,
    "max_retries": 3,
    "cost_optimization": {
      "max_requests_per_hour": 200,
      "max_daily_cost_usd": 5.0,
      "fallback_on_rate_limit": true,
      "use_local_for_simple_queries": true,
      "min_confidence_threshold": 0.75,
      "cache_similar_queries": true
    },
    "routing_preferences": {
      "prefer_local_models": true,
      "download_threshold_confidence": 0.85,
      "complex_query_threshold": 50,
      "enable_dynamic_prompts": true,
      "include_performance_history": true,
      "context_aware_routing": true
    },
    "prompt_optimization": {
      "include_model_specs": true,
      "include_previous_success": true,
      "include_download_status": true,
      "format_structured_response": true
    }
  },
  "gpu_constraints": {
    "max_vram_gb": 24,
    "preferred_model_sizes": [
      "3b",
      "7b",
      "8b",
      "9b",
      "13b"
    ],
    "avoid_sizes": [
      "70b",
      "72b",
      "90b",
      "405b"
    ]
  },
  "performance_tracking": true,
  "auto_download": true,
  "refresh_interval_hours": 12,
  "download_on_request": true,
  "ollama_config": {
    "host": "http://192.168.1.62:11434",
    "api_endpoint": "/api/generate",
    "health_endpoint": "/api/tags",
    "timeout_seconds": 30,
    "max_retries": 3,
    "connection_pool_size": 10,
    "verify_ssl": false,
    "headers": {
      "Content-Type": "application/json",
      "User-Agent": "AiSociety/1.0"
    }
  },
  "model_selection": {
    "strategy": "hybrid",
    "factors": {
      "specialization_match": 0.4,
      "performance_score": 0.3,
      "availability": 0.2,
      "resource_efficiency": 0.1
    },
    "auto_download_threshold": 0.8,
    "prefer_cached_models": true,
    "max_concurrent_downloads": 1
  },
  "local_model_boost": 1.1,
  "fallback_models": [
    "llama3.2:3b",
    "gemma2:2b",
    "phi3:mini"
  ],
  "download_recommendations": {
    "priority_models": {
      "coding": [
        {"name": "qwen2.5-coder:7b", "priority": 1, "size": "4.7GB"},
        {"name": "deepseek-coder-v2:16b", "priority": 2, "size": "9.0GB"},
        {"name": "codellama:7b", "priority": 3, "size": "3.8GB"}
      ],
      "math": [
        {"name": "qwen2.5:7b", "priority": 1, "size": "4.4GB"},
        {"name": "phi3:mini", "priority": 2, "size": "2.3GB"},
        {"name": "gemma2:9b", "priority": 3, "size": "5.4GB"}
      ],
      "general": [
        {"name": "llama3.2:3b", "priority": 1, "size": "2.0GB"},
        {"name": "mistral:7b", "priority": 2, "size": "4.1GB"},
        {"name": "neural-chat:7b", "priority": 3, "size": "4.1GB"}
      ],
      "creative": [
        {"name": "llama3.2:3b", "priority": 1, "size": "2.0GB"},
        {"name": "vicuna:7b", "priority": 2, "size": "3.8GB"},
        {"name": "yi:9b", "priority": 3, "size": "5.0GB"}
      ]
    },
    "essential": [
      "llama3.2:3b",
      "qwen2.5-coder:7b",
      "phi3:mini"
    ],
    "recommended": [
      "mistral:7b",
      "gemma2:9b",
      "qwen2.5:7b"
    ],
    "optional": [
      "command-r:35b",
      "yi:9b",
      "solar:10.7b"
    ]
  },
  "logging": {
    "level": "INFO",
    "log_routing_decisions": true,
    "log_performance_metrics": true,
    "log_openai_requests": true,
    "max_log_size_mb": 100
  }
}