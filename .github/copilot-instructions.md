# AI Society - Dynamic LLM Routing System

## ğŸ¯ Project Overview
A Python-based dynamic LLM routing system that democratizes AI access by intelligently routing queries to specialized local models. Uses 40-50% less power than monolithic models while maintaining excellent performance.

## ğŸ“ Complete Project Structure
```
ai-society/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ daemon/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ model_discovery.py          # Ollama library scanner & daemon
â”‚   â””â”€â”€ routing/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ intelligent_router.py       # Smart model selection logic
â”œâ”€â”€ web/
â”‚   â””â”€â”€ app.py                         # FastAPI web application
â”œâ”€â”€ config/
â”‚   â””â”€â”€ router_config.json             # System configuration
â”œâ”€â”€ data/                              # Auto-created: model cache & performance data
â”œâ”€â”€ venv/                              # Auto-created: virtual environment
â”œâ”€â”€ logs/                              # Auto-created: application logs
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ copilot-instructions.md        # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ setup.sh                          # Automated setup script (executable)
â”œâ”€â”€ start.sh                          # Quick start script (executable)  
â”œâ”€â”€ test_system.py                     # System testing script (executable)
â”œâ”€â”€ main.py                           # Alternative entry point
â””â”€â”€ README.md                         # Complete documentation
```

## ğŸš€ Quick Start Commands
```bash
# Setup (first time only)
./setup.sh

# Start system
./start.sh

# Test system
./test_system.py

# Manual start
python web/app.py
```

## ğŸ”§ Technical Architecture
- **Model Discovery Daemon**: Automatically scans Ollama library for latest models
- **Intelligent Router**: Analyzes queries and selects optimal specialized models  
- **Dynamic Downloads**: Automatically downloads best models on-demand
- **Power Optimization**: Targets 7B-13B models optimized for RTX 3090
- **Performance Tracking**: Monitors model performance for better future selections

## ğŸ¨ Web Interface Features
- Real-time WebSocket chat at http://localhost:8000
- Model transparency showing which model handled each query
- Performance metrics and response times
- Mobile responsive design
- API documentation at http://localhost:8000/docs

## ğŸ¤– Supported Model Types
- **Coding**: qwen2.5-coder:7b, codellama:7b
- **Math**: phi3:mini, qwen2.5:7b  
- **General**: llama3.2:3b, gemma2:9b
- **Reasoning**: mistral:7b, yi:9b
- **Conversation**: neural-chat:7b, vicuna:7b

## ğŸ”Œ API Endpoints
- `ws://localhost:8000/ws` - WebSocket chat
- `GET /api/health` - System health
- `GET /api/stats` - Performance stats
- `GET /api/models` - Available models
- `POST /api/refresh` - Refresh model registry
